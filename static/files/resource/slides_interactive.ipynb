{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"rise":{"start_slideshow_at":"beginning"},"colab":{"name":"slides_interactive.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"MeRY28Y2oXHH"},"source":["### an overly broad introduction to a selection of data science and machine learning topics, including&mdash;but not limited to&mdash;terminology; basic pipelines; and quantifying uncertainty, by someone who has worked on some but not all of this before\n","\n","\n","##### *this interactive slideshow was made with RISE*"]},{"cell_type":"markdown","metadata":{"id":"gHxHqEQVoXHe"},"source":["this notebook was written by wil ward"]},{"cell_type":"markdown","metadata":{"id":"pfRwc6-qoXHf"},"source":["- **some terminology**\n","- **more terminology**\n","- **types of data**\n","- **preparing data**\n","- **regression**\n","- **non-linear regression**\n","- **unsupervised learning**\n","- **key points**\n","- **anything else**"]},{"cell_type":"code","metadata":{"id":"_FEL4YzIoXHg"},"source":["# some imports\n","import numpy as np\n","import os, sys\n","\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","import seaborn\n","\n","seaborn.set_theme(\n","    context=\"talk\",\n","    rc={\"lines.linewidth\": 2},\n","    palette='colorblind',\n","    font_scale=1)\n","\n","import pandas as pd\n","\n","from PIL import Image\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PvU9bxUdoXHi"},"source":["## some terminology"]},{"cell_type":"markdown","metadata":{"id":"EHzBAhdHoXHj"},"source":["**data**\n","\n","a collection of information, typically numeric but possibly comprising words or images"]},{"cell_type":"markdown","metadata":{"id":"LT2NXEp6oXHj"},"source":["data are not their interpretation"]},{"cell_type":"markdown","metadata":{"id":"74KBV6w2oXHk"},"source":["**feature**\n","\n","a property of the data"]},{"cell_type":"markdown","metadata":{"id":"J3Zd3xg0oXHk"},"source":["**model**\n","\n","an abstract representation of a system used to make sense of data and its features"]},{"cell_type":"markdown","metadata":{"id":"4oqfyYGkoXHl"},"source":["**decision**\n","\n","the execution of a specific rule or choice given an input"]},{"cell_type":"markdown","metadata":{"id":"zQDbP2-4oXHm"},"source":["**prediction**\n","\n","a guess at what the features might show given specific conditions"]},{"cell_type":"markdown","metadata":{"id":"zCJm_PfWoXHm"},"source":["**inference / training**\n","\n","making a model represent the data and its features"]},{"cell_type":"markdown","metadata":{"id":"1hAtVwEyoXHn"},"source":["**objective function / loss**\n","\n","how close a model outputs are to the data features"]},{"cell_type":"markdown","metadata":{"id":"mAKzwE1loXHn"},"source":["**learning algorithm**\n","\n","the specific _choice_ of model(s) and training used to represent the data"]},{"cell_type":"markdown","metadata":{"id":"YLrwu3RUoXHo"},"source":["## what is ...\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dmxMa9hcoXHo"},"source":["### data science\n","\n","the study of acquiring knowledge from data, including acquisition; curation; analysis and visualisation"]},{"cell_type":"markdown","metadata":{"id":"hz1XjosToXHp"},"source":["### artificial intelligence\n","\n","a (computer) system that can make decisions and predictions"]},{"cell_type":"markdown","metadata":{"id":"LHn6YubLoXHp"},"source":["### machine learning\n","\n","the automatic development of _rules_ without explicit intervention"]},{"cell_type":"markdown","metadata":{"id":"SGS9vKNBoXHp"},"source":["### big data\n","\n","data that either\n","- consists of a huge number of data\n","- has lots of features"]},{"cell_type":"markdown","metadata":{"id":"nbm3OuAboXHq"},"source":["<center><img width=1200px src=\"https://wilocw.github.io/files/resource/global/datasci.png\"></img></center>"]},{"cell_type":"markdown","metadata":{"id":"DiFNI2F2oXHq"},"source":["## more terminology"]},{"cell_type":"markdown","metadata":{"id":"Ie7rVgVPoXHr"},"source":["#### data\n","\n","$X$ inputs (sometimes $t$ if time-series)\n","\n","$y$ outputs/observations"]},{"cell_type":"markdown","metadata":{"id":"5sCQOeRIoXHr"},"source":["#### model\n","\n","$\\theta$ parameters (these are learned)\n","\n","$\\alpha$ hyperparameters (these are fixed)\n","\n","$f$ learnable function, e.g. $f(X,\\theta;\\alpha)$\n","\n","$\\varepsilon$ (observation) noise, typically Gaussian"]},{"cell_type":"markdown","metadata":{"id":"qi44JyfioXHr"},"source":["#### model\n","\n","$\\mathcal{L}$ objective function\n","\n","$\\pi$ observation model / likelihood\n","\n","$z$ intermediate (latent) variable(s)\n","\n","example:\n","$$\n","    z = f(X,\\theta)\\qquad\\qquad\n","    y = \\pi(z)\n","$$\n","where $\\pi(z) = z + \\varepsilon$, and $\\varepsilon \\sim \\mathrm{N}(0,\\sigma^2)$"]},{"cell_type":"markdown","metadata":{"id":"3XQAAzEYoXHs"},"source":["## types of data"]},{"cell_type":"markdown","metadata":{"id":"ZwIKfEA3oXHs"},"source":["<div class='container'>\n","    \n","<div class='col' style='text-align: left'>\n","    <h3>continuous</h3>\n","    <br />\n","    time-series [$y = f(t)$]<br />\n","    spatial data [$y = f(\\mathbf{x})$]<br />\n","    spatio-temporal [$y = f(\\mathbf{x},t)$]\n","</div>\n","       \n","<div class='col' style='text-align: left'>\n","    <h3>discrete</h3><br />\n","    classification<br />\n","    count data [$y_k = \\lambda(x_k)$]<br />\n","    images [$y_k = f(I_k)$]<br />\n","    graphs / networks [$y = f(G(X),W)$]\n","</div>\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"Ux8hVyvFoXHt"},"source":["continuous functions are easier to model\n","\n","pretty much nothing is ever _measured_ continuously"]},{"cell_type":"markdown","metadata":{"id":"RXhuNZdSoXHt"},"source":["## preparing data"]},{"cell_type":"markdown","metadata":{"id":"co5KxC2CoXHu"},"source":["data quality has direct impact on analysis and predictions\n","\n","curation and storage important factors\n","\n","considerations include privacy, access, visualisation"]},{"cell_type":"markdown","metadata":{"id":"67FZBplcoXHu"},"source":["few established best practices\n","\n","outlier detection, dimensionality reduction, summary statistics\n","\n","data readiness a contemporary issue:<br/> [The DELVE Initiative (2020), Data Readiness: Lessons from an Emergency. DELVE Report No. 7. Published 24 November 2020](https://rs-delve.github.io/reports/2020/11/24/data-readiness-lessons-from-an-emergency.html#key-points)"]},{"cell_type":"markdown","metadata":{"id":"BlAfd0hjoXHu"},"source":["DELVE = Data Evaluation and Learning for Viral Epidemics\n","\n","A multi-disciplinary group, convened by the Royal Society, to support a data-driven approach to learning from the different approaches countries are taking to managing the covid-19 pandemic. \n","\n","The report builds on the experience of the Royal Societyâ€™s DELVE group in providing rapid turnaround, data-driven answers to policy questions, and the challenges in access to data that arose during this work. It sets out the challenges we found that were common to these data sharing activities, focussing on the use of new data modalities, arising from, for example mobile phone usage or electronic payments data, rather than the classical challenges of collecting surveillance data. It then considers recommendations for addressing these challenges and creating an environment that supports the safe and rapid use of a range of different types of data in policymaking."]},{"cell_type":"markdown","metadata":{"id":"kV1ZEKlYoXHv"},"source":["### example"]},{"cell_type":"code","metadata":{"id":"ix7pYjQ_oXHv"},"source":["data = pd.read_csv('https://wilocw.github.io/files/resource/global/2019.csv')\n","data.rename(lambda s: s.replace(' ','_'), axis='columns', inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ow1eQX7GoXHw"},"source":["data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"McAxiDI5oXHx"},"source":["_World Happiness Report, 2019. UN Sustainable Development Solutions Network_.<br/>https://www.kaggle.com/unsdsn/world-happiness/metadata"]},{"cell_type":"markdown","metadata":{"id":"Y8K6YUyRoXHx"},"source":["#### extract features"]},{"cell_type":"code","metadata":{"id":"_JhBmb5eoXHx"},"source":["features = ['GDP_per_capita', 'Social_support', 'Life_expectancy', 'Freedom', 'Generosity', 'Corruption']\n","\n","X = data.loc[:,features].values\n","\n","y = data.loc[:,['Score']].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"shmDTzokoXHx"},"source":["seaborn.pairplot(data, vars=features, hue='Continent', diag_kind='kde', plot_kws={'s':100});"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ml419q1voXHx"},"source":["#### data standardisation\n","\n","normalise features to give consistent statistical properties\n","\n","$$ \\frac{X - \\mathrm{mean}(X)}{\\mathrm{std}(X)} $$"]},{"cell_type":"code","metadata":{"id":"oNgYnC2HoXHy"},"source":["X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0))\n","\n","print('standardised mean: %2.1f \\nstandardised std: %2.1f' % (np.mean(X),np.std(X)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QOObnxpPoXHy"},"source":["#### dimensionality reduction\n","\n","reduce number of relative features by taking into account internal correlations\n","\n","use principle component analysis (PCA)"]},{"cell_type":"code","metadata":{"id":"iodfhn65oXHy"},"source":["from sklearn.decomposition import PCA\n","\n","def plot(pc):\n","    ''' this is a utility function to plot one or two principle components '''\n","    if pc.shape[1] == 1:\n","        df = pd.DataFrame({'pc1': pc.ravel(), 'Continent': data.loc[:,['Continent']].values.ravel(), '0': np.zeros_like(pc.ravel())})\n","\n","        p = seaborn.FacetGrid(df, aspect=8, height=2)\n","        p.map_dataframe(seaborn.scatterplot, x='pc1', y='0', hue='Continent', s=200)\n","        p.add_legend()\n","        p.set_axis_labels('pc1','')\n","        p.set(yticks=[]);\n","    else:\n","        df = pd.DataFrame({'pc1': pc[:,0], 'pc2': pc[:,1], 'Continent': data.loc[:,['Continent']].values.ravel()})\n","\n","        p = seaborn.FacetGrid(df, height=7)\n","        p.map_dataframe(seaborn.scatterplot, x='pc1', y='pc2', hue='Continent', s=200)\n","        p.set_axis_labels('pc1','pc2')\n","        p.add_legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kw8duLrvoXHy"},"source":["z = PCA(1).fit_transform(X)\n","plot(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"UjrvRo2aoXHz"},"source":["z = PCA(2).fit_transform(X)\n","plot(z)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wuWwu7qGoXHz"},"source":["dimensionality reduction finds a _latent_ representation to represent data in lower number of features\n","\n","an example of unsupervised learning\n","\n","no free lunch\n","\n","other examples: _autoencoders_, _GP-LVM_, _neural networks_"]},{"cell_type":"markdown","metadata":{"id":"yHbQ4xVfoXHz"},"source":["## regression"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"LAC-LHK_oXHz"},"source":["### this is an example of linear regression on one feature against the \"happiness\" score\n","\n","def regression_example():\n","    ''' this function will create an interactive regression tool'''\n","    \n","    # reinitialise data \n","    X, y = data.loc[:,features].values, data.loc[:,['Score']].values\n","    # standardise\n","    X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0))\n","    y = (y - np.mean(y)) / np.std(y)\n","\n","    # dummy X for plotting a straight line (only need end points)\n","    dummy_x = np.array([[-2.5],[2.2]])\n","\n","    def reset_ax1(x,y):\n","        ''' this will reset the right hand axis (interactive) '''\n","        axs[1].clear()\n","\n","        axs[1].plot([-2, x], [y,y], 'r-')\n","        axs[1].plot([x,x], [-2, y], 'r-')\n","\n","        axs[1].text(x - 1, y+0.1,'%5.4f, %5.4f' % (x, y))\n","\n","        axs[1].set_xlim([-2,2])\n","        axs[1].set_ylim([-2,2])\n","        \n","        axs[1].set_xlabel(r'$\\theta_0$')\n","        axs[1].set_ylabel(r'$\\theta_1$')\n","\n","    def onpick(event):\n","        ''' this will update (and plot) the fit with the selected parameters '''\n","        if event.x < 450: \n","            return\n","        # reset left hand plot\n","        axs[0].clear()\n","        plot_gdpexp(axs[0])\n","        # plot f(x) = t0 + t1*x\n","        dummy_y = event.ydata*dummy_x + event.xdata\n","        axs[0].plot(dummy_x, dummy_y, 'm-', label=r'$\\theta_0 + \\theta_1x$')\n","        # misc. details\n","        axs[0].legend(loc='lower right')\n","        axs[0].set_title(r'$\\mathcal{L} = %5.3f$'%r2(event.xdata,event.ydata))\n","        reset_ax1(event.xdata, event.ydata)\n","\n","    def plot_gdpexp(ax):\n","        ''' this just plots the training data '''\n","        ax.plot(X[:,0], y,'.', label=None)\n","        ax.set_xlim([-2.5, 2.2])\n","        ax.set_ylim([-2.54, 2.54])\n","\n","        axs[0].set_xlabel(features[0])\n","        axs[0].set_ylabel('Score')\n","\n","    def r2(t0,t1):\n","        ''' square error '''\n","        f = X[:,0]*t1 + t0\n","        return np.sum((y.ravel()-f)**2)\n","    \n","    # create a plot\n","    fig, axs = plt.subplots(1,2, figsize=(14, 6))\n","    # plot the data\n","    plot_gdpexp(axs[0])\n","    # plot the default straight like f(x) = 0 + 0x\n","    axs[0].plot(dummy_x, dummy_x*0 + 0, 'm-', label=r'$\\theta_0 + \\theta_1x$')\n","    axs[0].legend(loc='lower right')\n","    axs[0].set_title(r'$\\mathcal{L} = %5.3f$'%r2(0,0))\n","    # initialise (reset) the right hand plot\n","    reset_ax1(0,0)\n","    # add callback to mouse click on axes\n","    fig.canvas.mpl_connect(\"button_press_event\", onpick)\n","    plt.tight_layout()\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pj4CE84foXH0"},"source":["### linear regression\n","\n","$$ f(x, \\theta) =  \\theta_0 + \\theta_1x $$\n","\n","minimise $$\\mathcal{L} = \\sum (y - f(X, \\theta))^2$$"]},{"cell_type":"code","metadata":{"id":"ksXBlcBKoXH0"},"source":["%matplotlib notebook\n","regression_example()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EuSOQLG-oXH0"},"source":["<center><h2>machine learning is optimisation of statistical metrics</h2></center>"]},{"cell_type":"markdown","metadata":{"id":"IDFtZs4SoXH1"},"source":["$$\n","\\text{find } f^* \\in \\mathcal{F} \\text{ such that}\\qquad\\qquad\\qquad\n","$$\n","\n","$$\n","    f^* = \\arg\\min_{f\\in\\mathcal{F}} \\mathcal{L}(y,f(X))\n","$$"]},{"cell_type":"markdown","metadata":{"id":"-XEBnedRoXH1"},"source":["<center><h2>machine learning is optimisation of statistical metrics</h2></center>\n","\n","\n","$$\n","\\text{find } \\theta^* \\in \\Theta \\text{ such that}\\qquad\\qquad\\qquad\n","$$\n","\n","$$\n","    \\theta^* = \\arg\\min_{\\theta\\in\\Theta} \\mathcal{L}(y,f(X,\\theta))\n","$$"]},{"cell_type":"markdown","metadata":{"id":"gcO07aCfoXH1"},"source":["### training a regressor"]},{"cell_type":"code","metadata":{"id":"W8tgzbzVoXH1"},"source":["from scipy.optimize import minimize\n","from matplotlib.colors import LogNorm\n","\n","def trained_regression_example():\n","    ''' this will automatically fit a curve and plot the learning path '''\n","    \n","    def callback(itr):\n","        # save the optimiser step for plotting later\n","        ts.append(itr)\n","        \n","    # reinitialise data\n","    X, y = data.loc[:,features].values, data.loc[:,['Score']].values\n","    # standardise\n","    X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0))\n","    y = (y - np.mean(y)) / np.std(y)\n","    \n","    # dummy X for plotting a straight line (only need end points)\n","    dummy_x = np.array([[-2.5],[2.2]])\n","    \n","    # resolution for plotting loss fcn (res x res)\n","    res = 100\n","    t0,t1 = np.meshgrid(np.linspace(-2,2,res),np.linspace(-2,2,res))\n","\n","    def r2(t):\n","        ''' square error '''\n","        f = X[:,0]*t[1] + t[0]\n","        return np.sum((y.ravel()-f)**2)\n","\n","    # calculate the full loss surface (this is easy for 2 parameters, but non-trivial usually)\n","    L_ = np.array([[r2([t0[i,j],t1[i,j]]) for j in range(res)] for i in range(res)])\n","   \n","    # initial estimate (arbitrary)\n","    t_0 = np.array([1,-1])\n","    # save optimiser guesses in ts\n","    ts = [t_0]\n","    \n","    # bounded minimisation of r2(theta)\n","    opt = minimize(r2, t_0, method='L-BFGS-B', bounds=((-2,2),(-2,2)), callback=callback)\n","\n","    def plot_gdpexp(ax):\n","        ''' this just plots the training data '''\n","        ax.plot(X[:,0], y,'.', label=None)\n","        ax.set_xlim([-2.5, 2.2])\n","        ax.set_ylim([-2.54, 2.54])\n","\n","        axs[0].set_xlabel(features[0])\n","        axs[0].set_ylabel('Score')\n","    \n","    # create a plot\n","    fig, axs = plt.subplots(1,2, figsize=(14,6))\n","    # plot the data\n","    plot_gdpexp(axs[0])\n","    \n","    # for each optimiser step, plot the fit\n","    for t in ts[::-1]:\n","        axs[0].plot(dummy_x, dummy_x*t[1] + t[0], 'k-', alpha=0.1)\n","    # plot final guess\n","    axs[0].plot(dummy_x, dummy_x*ts[-1][1] + ts[-1][0], 'm-')\n","    \n","    # on r.h. plot, show the loss surface\n","    axs[1].pcolor(t0, t1, L_, norm=LogNorm(vmin=L_.min(), vmax=L_.max()))\n","    # ... and the optimiser search path\n","    axs[1].plot([t[0] for t in ts], [t[1] for t in ts], 'ro-')\n","    \n","    # misc. details\n","    axs[1].set_xlim([-2,2])\n","    axs[1].set_ylim([-2,2])\n","    axs[1].set_xlabel(r'$\\theta_0$')\n","    axs[1].set_ylabel(r'$\\theta_1$')\n","    \n","    plt.tight_layout()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TR-YrsFuoXH2"},"source":["%matplotlib inline\n","trained_regression_example()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zEnU491-oXH2"},"source":["### a Bayesian example"]},{"cell_type":"markdown","metadata":{"id":"JQOdJMJKoXH3"},"source":["prior: $p(\\theta) = \\mathrm{N}(\\theta\\,|\\,\\mathbf{0},\\mathbf{I})$\n","\n","likelihood: $p(y\\,|\\,X,\\theta) = \\mathrm{N}(y\\,|\\, \\theta_0 + \\theta_1X, \\sigma^2)$"]},{"cell_type":"markdown","metadata":{"id":"J6hdHH0loXH4"},"source":["we are interested in an estimate of $\\theta$ conditioned on the data. the posterior: $p(\\theta\\,|\\,X,y)$"]},{"cell_type":"markdown","metadata":{"id":"EDGLDNIToXH5"},"source":["### prior knowledge"]},{"cell_type":"code","metadata":{"id":"G8sl1xP5oXH5"},"source":["from scipy.stats import multivariate_normal as mvn\n","\n","def plot_prior():\n","    ''' this will plot fits based on samples from the prior of theta '''\n","    # reinitialise the data\n","    X, y = data.loc[:,features].values, data.loc[:,['Score']].values\n","    # standardise\n","    X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0))\n","    y = (y - np.mean(y)) / np.std(y)\n","\n","    # prior p(theta) = N(0, I) [mvn = multivariate normal]\n","    p_t = mvn(np.zeros(2,), np.eye(2))\n","    \n","    # dummy X for plotting a straight line (only need end points)\n","    dummy_x = np.array([[-2.5],[2.2]])\n","    \n","    # resolution for plotting prior (res x res)\n","    res = 100\n","    t0,t1 = np.meshgrid(np.linspace(-2,2,res),np.linspace(-2,2,res))\n","    \n","    def prior(ax):\n","        ''' contour plot of the prior p_t '''\n","        ax.contour(t0,t1, np.array([[p_t.pdf([t0[i,j],t1[i,j]]) for j in range(res)] for i in range(res)]),'k-', levels=5)\n","        \n","    def plot_gdpexp(ax):\n","        ''' this just plots the training data '''\n","        ax.plot(X[:,0], y,'.', label=None)\n","        ax.set_xlim([-2.5, 2.2])\n","        ax.set_ylim([-2.54, 2.54])\n","\n","        axs[0].set_xlabel(features[0])\n","        axs[0].set_ylabel('Score')\n","    \n","    # create a plot\n","    fig, axs = plt.subplots(1,2, figsize=(14,6))\n","    # plot the data\n","    plot_gdpexp(axs[0])\n","    \n","    # draw 100 random variable samples (rvs) from the prior p(theta)\n","    ts = p_t.rvs(100)\n","    # for each sample of theta, plot the corresponding linear fit\n","    for i in range(100):\n","        axs[0].plot(dummy_x, dummy_x*ts[i,:][1] + ts[i,:][0], 'k-', alpha=0.2,lw=0.5)\n","    \n","    # plot the prior on the r.h. plot\n","    prior(axs[1])\n","    # plot the samples\n","    axs[1].plot(ts[:,0],ts[:,1],'k.',ms=5)\n","    \n","    # misc. details\n","    axs[1].set_title(r'$p(\\theta)$')\n","    axs[1].set_xlim([-2,2])\n","    axs[1].set_ylim([-2,2])\n","    axs[1].set_xlabel(r'$\\theta_0$')\n","    axs[1].set_ylabel(r'$\\theta_1$')\n","    \n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HtRNZKnpoXH7"},"source":["%matplotlib inline\n","plot_prior()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8boGxdZkoXH-"},"source":["def plot_posterior(s2=1.):\n","    ''' this will plot fits based on posterior p(theta|X,y) based on observation noise s2 '''\n","    # reinitialise the data\n","    X, y = data.loc[:,features].values, data.loc[:,['Score']].values\n","    # standardise\n","    X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0))\n","    y = (y - np.mean(y)) / np.std(y)\n","    \n","    # dummy X for plotting a straight line (only need end points)\n","    dummy_x = np.array([[-2.5],[2.2]])\n","    \n","    ## we add a dummy variable of 1s so we can do some linear algebra\n","    x_ = np.vstack((np.ones_like(X[:,0]), X[:,0])).T\n","    \n","    # resolution for plotting prior (res x res)\n","    res=100\n","    t0,t1 = np.meshgrid(np.linspace(-2,2,res),np.linspace(-2,2,res))\n","    \n","    # inferred posterior (this is a closed form calculation [not really ML])\n","    def p_t_given_xy():\n","        ''' the posterior p(theta|X,y) based on obs. noise s2 '''\n","        Sig = np.linalg.inv(np.eye(2) + (x_.T@x_)/s2)\n","        mu = Sig@np.sum(y*x_, axis=0)/s2\n","        return mvn(mu,Sig) # N(mu, Sig)\n","\n","    def posterior(ax):\n","        ''' contour plot of the prior p_t '''\n","        ax.contour(t0,t1, np.array([[p_t_given_xy().pdf([t0[i,j],t1[i,j]]) for j in range(res)] for i in range(res)]),'k-', levels=5)\n","        \n","    # this is the negative log likelihood of our estimate (the lower the better !)\n","    negloglik = lambda s: -mvn(np.zeros_like(y).ravel(), (x_@x_.T) + s*np.eye(y.shape[0])).logpdf(y.ravel())\n","    \n","    def plot_gdpexp(ax):\n","        ''' this just plots the training data '''\n","        ax.plot(X[:,0], y,'.', label=None)\n","        ax.set_xlim([-2.5, 2.2])\n","        ax.set_ylim([-2.54, 2.54])\n","\n","        axs[0].set_xlabel(features[0])\n","        axs[0].set_ylabel('Score')\n","        axs[0].set_title(r'$\\sigma^2$=%5.4f  ; $\\mathcal{L}(\\sigma^2)$ = %5.4f' % (s2,negloglik(s2)))\n","    \n","    # create a plot\n","    fig, axs = plt.subplots(1,2, figsize=(14,6))\n","    # plot the data\n","    plot_gdpexp(axs[0])\n","    \n","    # draw 100 random variable samples (rvs) from the posterior p(theta | X,y)\n","    ts = p_t_given_xy().rvs(100)\n","    # for each sample of theta, plot the corresponding linear fit\n","    for i in range(100):\n","        axs[0].plot(dummy_x, dummy_x*ts[i,:][1] + ts[i,:][0], 'k-', alpha=0.2,lw=0.5)\n","\n","    # plot the posterior in the r.h. plot\n","    posterior(axs[1])\n","    # plot the samples of theta\n","    axs[1].plot(ts[:,0],ts[:,1],'k.',ms=5)\n","    \n","    # misc. details\n","    axs[1].set_title(r'$p(\\theta\\,|\\,X,y)$')\n","    axs[1].set_xlim([-2,2])\n","    axs[1].set_ylim([-2,2])\n","    axs[1].set_xlabel(r'$\\theta_0$')\n","    axs[1].set_ylabel(r'$\\theta_1$')\n","    \n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-qvX2m6oXH_"},"source":["%matplotlib inline\n","plot_posterior(s2 = 1.)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M4CNUoJwoXIB"},"source":["the next lines of code optimise the observation noise to jointly learn all parameters !"]},{"cell_type":"code","metadata":{"id":"ImE7RYY3oXIC"},"source":["x_ = np.vstack((np.ones_like(X[:,0]), X[:,0])).T\n","# this is the negative log likelihood of our estimate (the lower the better !)\n","negloglik = lambda s: -mvn(np.zeros_like(y).ravel(), (x_@x_.T) + s*np.eye(y.shape[0])).logpdf(y.ravel())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cYTE0O-PoXIC"},"source":["opt = minimize(negloglik, [1.], method='L-BFGS-B', bounds=((1e-5, 10.),))\n","\n","plot_posterior(opt.x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I6Ny6Ya8oXID"},"source":["def plot_linear_errorbars(s2=1.):\n","    ''' this will plot the uncertainty (95% CI) on posterior p(theta|X,y) based on observation noise s2 '''\n","    \n","    # reinitialise the data\n","    X, y = data.loc[:,features].values, data.loc[:,['Score']].values\n","    # standardise\n","    X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0))\n","    y = (y - np.mean(y)) / np.std(y)\n","    \n","    ## we add a dummy variable of 1s so we can do some linear algebra\n","    x_ = np.vstack((np.ones_like(X[:,0]), X[:,0])).T\n","    \n","    # resolution for plotting prior (res x res)\n","    res=100\n","    t0,t1 = np.meshgrid(np.linspace(-2,2,res),np.linspace(-2,2,res))\n","    \n","    # inferred posterior (this is a closed form calculation [not really ML])\n","    def p_t_given_xy():\n","        ''' the posterior p(theta|X,y) based on obs. noise s2 '''\n","        Sig = np.linalg.inv(np.eye(2) + (x_.T@x_)/s2)\n","        mu = Sig@np.sum(y*x_, axis=0)/s2\n","        return mvn(mu,Sig) # N(mu, Sig)\n","    \n","    def plot_gdpexp(ax):\n","        ''' this just plots the training data '''\n","        ax.plot(X[:,0], y,'.', label=None)\n","        ax.set_xlim([-2.5, 2.2])\n","        ax.set_ylim([-2.54, 2.54])\n","\n","        ax.set_xlabel(features[0])\n","        ax.set_ylabel('Score')\n","        ax.set_title(r'$\\sigma^2$=%5.4f  ; $\\mathcal{L}(\\sigma^2)$ = %5.4f' % (s2,negloglik(s2)))\n","\n","    # create a plot\n","    fig, ax = plt.subplots(1,1, figsize=(7,6))\n","    \n","    # we take lots of samples to do Monte Carlo estimates of uncertainty\n","    ts = p_t_given_xy().rvs(1000)    \n","    # resolution of fit (we need this to get estimates of uncertainty all the way across the fit)\n","    ln = 200\n","    # dummy X for plotting a straight line (need more points)\n","    dummy_x = np.linspace(-2.5, 2.2, ln)\n","    # calculate fits for each sample of theta\n","    dummy_y = np.vstack([dummy_x*ts[i,:][1] + ts[i,:][0] for i in range(1000)])\n","    \n","    # calculate 95% CI (2.5, 97.5 quantiles) of fits\n","    qnt = np.quantile(dummy_y, q=(0.025, 0.975), axis=0)\n","    # plot 95% CI with observation noise\n","    ax.fill_between(dummy_x, qnt[0,:] - 1.96*s2, qnt[1,:] + 1.96*s2, alpha=0.25, label='observation 95% CI')\n","    # plot 95% CI without observation noise (effects of theta only)\n","    ax.fill_between(dummy_x, qnt[0,:], qnt[1,:], alpha=1., label='latent 95% CI')\n","    \n","    # calculate the mean fit\n","    mu = p_t_given_xy().mean\n","    # plot the mean fit\n","    ax.plot(dummy_x, dummy_x*mu[1] + mu[0], 'k-', lw=2, label=r'$\\mathbf{E}[y\\,|\\theta]$')\n","\n","    # plot the data\n","    plot_gdpexp(ax)\n","    \n","    # add a legend\n","    ax.legend()\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zL1ZJnJMoXIE"},"source":["plot_linear_errorbars(opt.x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d89BXygXoXIE"},"source":["### generalised linear models\n","\n","$$ f(X,\\theta) = \\theta_0 + \\theta_1X_1 + \\theta_2X_2 + \\ldots$$\n","\n","$$\\theta \\sim p(\\theta)$$\n","$$y \\sim \\pi(f(X,\\theta),\\alpha)$$"]},{"cell_type":"markdown","metadata":{"id":"QApV41vRoXIF"},"source":["this example is a bit more involved, computationally, and requires using Markov chain Monte Carlo (MCMC) -- skip it if you like"]},{"cell_type":"code","metadata":{"id":"pN1okSh6oXIG"},"source":["import pymc3 as pm\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","def plot_glm():\n","    ''' calculate and plot the generalised linear model '''\n","    \n","    ## reload and reinitialise all the data (just in case)\n","    data = pd.read_csv('https://wilocw.github.io/files/resource/global/2019.csv')\n","    data.rename(lambda s: s.replace(' ','_'), axis='columns', inplace=True)\n","\n","    data = pd.DataFrame(StandardScaler().fit_transform(data[['Score'] + features]), columns=['Score'] + features)\n","\n","    # GLM formula \"Score ~ GDP_per_capita + Social_support + Life_expectancy + Freedom + Generosity + Corruption\"\n","    formula = 'Score ~' + ''.join([' ' + feature + ' +' for feature in features])[:-2]\n","\n","    # context for the model\n","    with pm.Model() as normal_model:\n","\n","        # the prior for the likelihood\n","        family = pm.glm.families.Normal()\n","\n","        # creating the model requires a formula and data (and optionally a family)\n","        pm.GLM.from_formula(formula, data = data, family = family)\n","\n","        # perform Markov chain Monte Carlo sampling\n","        normal_trace = pm.sample(draws=2000, chains = 2, tune = 1000)\n","        # sample the posterior for features and intercept\n","        posterior_predictive = pm.sample_posterior_predictive(normal_trace, var_names=features + ['Intercept'])\n","\n","    # calculate the median value of posterior\n","    medians = {feature: np.median(posterior_predictive[feature]) for feature in features}\n","    \n","    # create a plot\n","    fig, axs = plt.subplots(2,3, figsize=(14, 12))\n","\n","    # for each feature, plot the linear fit\n","    for i, feature in enumerate(features):\n","        # index for plots (equiv to ind2sub in matlab)\n","        j,k = i // 3, i % 3\n","        # get the current feature\n","        x_ = data[feature]\n","    #     axs[j,k].plot(x_, data['Score'],'.', label=None) # its actually not great a fit so this is commented out for the talk\n","        axs[j,k].set_xlabel(feature)\n","        if k == 0:\n","            axs[j,k].set_ylabel('Score')\n","        # For a subset posterior sample (1000 is too many to plot), fix the other features (to median) and create a fit\n","        for t0,t1 in zip(posterior_predictive['Intercept'][::40],posterior_predictive[feature][::40]):\n","            f = t0 + t1*x_ + np.sum([data[nfeature]*medians[nfeature] for nfeature in features if nfeature is not feature])\n","            axs[j,k].plot(x_, t0 + t1*x_, 'k-', alpha=0.2, lw=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIJRRQpsoXIG"},"source":["plot_glm()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VP6JIZAqoXIH"},"source":["### would it be easier to predict trends via PCA ?"]},{"cell_type":"code","metadata":{"id":"Ffnj7iraoXIH"},"source":["def plot_pc_score(z):  \n","    ''' this function plots the princple components against y (Score) '''\n","    # create plots\n","    _, axs = plt.subplots(1,2,figsize=(14, 6), sharex=True, sharey=True)\n","    # plot pc1 vs score\n","    axs[0].plot(z[:,0], y, 'o')\n","    axs[0].set_xlabel('pc1')\n","    axs[0].set_ylabel('score')\n","    # plot pc2 vs score\n","    axs[1].plot([],[],'o')\n","    axs[1].plot(z[:,1], y, 'o')\n","    axs[1].set_xlabel('pc2')\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mS34szsKoXIH"},"source":["z = PCA(2).fit_transform(X)\n","plot_pc_score(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-e2Y45qhoXII"},"source":["## exercise left to the reader ! fit a linear model to the PCA transformation and \n","\n","data = pd.read_csv('https://wilocw.github.io/files/resource/global/2019.csv')\n","data.rename(lambda s: s.replace(' ','_'), axis='columns', inplace=True)\n","\n","X = StandardScaler().fit_transform(data.loc[:,features].values)\n","z = PCA(2).fit_transform(X)\n","\n","# ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SKOWkbxvoXII"},"source":["## nonlinear regression"]},{"cell_type":"markdown","metadata":{"id":"cs0AjoWDoXIJ"},"source":["### nonlinear regression\n","\n","most functions and models aren't linear"]},{"cell_type":"markdown","metadata":{"id":"jFSzww1BoXIJ"},"source":["(but sometimes it's easier to pretend they are)"]},{"cell_type":"markdown","metadata":{"id":"fJ7B6XbloXIJ"},"source":["a popular method is to linearise data"]},{"cell_type":"markdown","metadata":{"id":"cht3IoheoXIK"},"source":["other methods include polynomial fitting; Gaussian processes; and neural networks"]},{"cell_type":"markdown","metadata":{"id":"E_LKLTlhoXIK"},"source":["### general regression framework\n","\n","$ z = f(X)$ ; $y \\sim \\pi(z)$ ; $f \\in \\mathcal{F}$\n","\n","$$\n","\\text{find } f^* \\in \\mathcal{F} \\text{ such that}\\qquad\\qquad\\qquad\n","$$\n","\n","$$\n","    f^* = \\arg\\min_{f\\in\\mathcal{F}} \\mathcal{L}(y,\\pi(z))\n","$$"]},{"cell_type":"markdown","metadata":{"id":"uN_I0kKMoXIK"},"source":["choice of $\\mathcal{F}$, $\\pi$ and $\\mathcal{L}$ define model"]},{"cell_type":"markdown","metadata":{"id":"tCdEcLCKoXIL"},"source":["choice of function space, likelihood (observation model), and objective metric define model"]},{"cell_type":"code","metadata":{"id":"2DMc6BELoXIL"},"source":["# set a random seed to get data \n","np.random.seed(1234)\n","\n","# generative function\n","f = lambda x: (10*np.cos(np.pi*x)/3 + np.sin(np.pi*x*2) + x**3 + 4*x*x)/20\n","\n","# create a mesh for plotting functions\n","nX = np.linspace(-2,2, 250)\n","# random X\n","X = np.random.choice(nX, (20))[:,None]\n","# y = f(X) + noise\n","y = f(X) + 0.1*np.random.randn(20,1)\n","\n","def plot_nl_data():\n","    ''' plot the training data '''\n","    plt.figure(figsize=(14, 5))\n","    plt.plot(X, y, 'o')\n","    plt.xlabel(r'$X$')\n","    plt.ylabel(r'$y$')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r52i6eCFoXIL"},"source":["%matplotlib inline\n","plot_nl_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i2ZsIzN7oXIM"},"source":["### example\n","\n","polynomial regression"]},{"cell_type":"markdown","metadata":{"id":"aGHDh_zBoXIM"},"source":["$$f(X,\\theta) = \\theta_0 + \\theta_1X + \\theta_2X^2 + \\ldots$$"]},{"cell_type":"markdown","metadata":{"id":"VIyLpPAxoXIM"},"source":["$$\\pi(z) = z + \\epsilon$$"]},{"cell_type":"markdown","metadata":{"id":"e6wFNxx2oXIN"},"source":["$$ \\mathcal{L} = \\sum \\epsilon^2 $$"]},{"cell_type":"code","metadata":{"id":"xoLKYri5oXIN"},"source":["from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LinearRegression\n","\n","def fit_polys():\n","    ''' fit polynomials with different orders '''\n","    \n","    # create a plot\n","    _, axs = plt.subplots(1,3,figsize=(16, 4), sharey=True)\n","    # for degree = 1, 4, and 10\n","    for i,d in enumerate([1,4,10]):\n","        # build the training pipeline in sklearn\n","        pipeline = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree=d, include_bias=True)),\n","                             (\"linear_regression\", LinearRegression())])\n","        # fit the data\n","        pipeline.fit(X,y)\n","        # plot the training data\n","        axs[i].plot(X, y, 'o')\n","        # plot the the trained curve\n","        axs[i].plot(nX, pipeline.predict(nX[:,None]),'-', label='num. params '+str(d+1))\n","        # misc. details\n","        axs[i].set_xlabel('X')\n","        axs[i].legend(loc='upper left')\n","        axs[i].set_ylim((-2.1,2.1))\n","    axs[0].set_ylabel('y')\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQ3_oGsWoXIN"},"source":["%matplotlib inline\n","fit_polys()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yaAETRProXIO"},"source":["parameters $\\theta$ are polynomial coefficients\n","\n","hyperparameters $\\alpha$ is polynomial order (num. params - 1)"]},{"cell_type":"markdown","metadata":{"id":"fidTcaEeoXIO"},"source":["### example\n","\n","Gaussian processes"]},{"cell_type":"markdown","metadata":{"id":"6Qkkl2yroXIO"},"source":["$$f(X) \\sim \\mathrm{GP}(m(X), k(X,X'))$$"]},{"cell_type":"markdown","metadata":{"id":"JFprid6BoXIP"},"source":["$$\\pi(z) = \\mathcal{N}(z, \\sigma^2)$$"]},{"cell_type":"markdown","metadata":{"id":"GG6qb6CooXIP"},"source":["$$ \\mathcal{L} = -\\log p(y\\,|\\,X,\\alpha) $$"]},{"cell_type":"code","metadata":{"id":"o5A2PNbtoXIP"},"source":["# sklearn is not a great library for GPs but it's easy enough to use here\n","import sklearn.gaussian_process as gp \n","from sklearn.gaussian_process.kernels import Matern,ConstantKernel, DotProduct, WhiteKernel\n","\n","def fit_gps():\n","    ''' we will fit some different Gaussian processes to the data '''\n","    \n","    # create a plot\n","    _, axs = plt.subplots(3,3,figsize=(16, 13), sharey=True, sharex=True)\n","    \n","    # different covariance assumptions\n","    kernels = [\n","        DotProduct(sigma_0=0.1) , # this is equivalent to Bayesian linear regression !\n","        ConstantKernel() + Matern(length_scale=0.01, nu=1/2), # this assumes similarity (covariance) decays exponentially\n","        ConstantKernel() + Matern(length_scale=10., nu=5/2)   # more smooth assumption of latent functions\n","    ]\n","    # for each kernel\n","    for i,k in enumerate(kernels):\n","        # fit a GP model with kernel k (assumes fixed [known] observation noise)\n","        gpr = gp.GaussianProcessRegressor(kernel=k, alpha=0.01, n_restarts_optimizer=10)\n","        gpr.fit(X,y)\n","        \n","        # we extract a mean and covariance for our latent GP\n","        m, S = gpr.predict(nX[:,None], return_cov=True)\n","        # ... from which we can construct a posterior of f(X)\n","        p_f = mvn(m.ravel(),S+1e-7*np.eye(len(m)))\n","        \n","        # plot the data and the mean fit\n","        axs[0,i].plot(X, y, 'o')\n","        axs[0,i].plot(nX, m, '-', lw=2, label=r'$\\mathbb{E}[f]$')\n","        axs[0,i].set_title(['Linear', 'Exponential', 'Matern 5/2'][i])\n","        axs[0,i].legend(loc='upper left')\n","        \n","        # plot samples from the posterior (most representative of the function estimates)\n","        axs[1,i].plot(X, y, 'o')\n","        axs[1,i].plot(nX, p_f.rvs(1).T, 'k-', lw=0.5, alpha=0.5, label=r'samples of $f$')\n","        axs[1,i].plot(nX, p_f.rvs(9).T,'k-', lw=0.5, alpha=0.5, label=None)\n","        axs[1,i].legend(loc='upper left')\n","        \n","        # plot the 95% CI of the latent and observation models\n","        axs[2,i].fill_between(nX, m.ravel() - 1.96*(np.sqrt(np.diag(S+0.01))), m.ravel() + 1.96*(np.sqrt(np.diag(S+0.01))), alpha=0.25, label='observation 95% CI')\n","        axs[2,i].fill_between(nX, m.ravel() - 1.96*np.sqrt(np.diag(S)), m.ravel() + 1.96*np.sqrt(np.diag(S)), alpha=1., label='latent 95% CI')\n","        axs[2,i].plot(X, y, 'o')\n","        axs[2,i].legend()\n","    \n","    # misc. details\n","    for i in range(3):\n","        axs[i,0].set_ylabel('y')\n","        axs[2,i].set_xlabel('X')\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvF4arW2oXIP"},"source":["%matplotlib inline\n","fit_gps()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7yHi0U_oXIQ"},"source":["### example\n","\n","neural network"]},{"cell_type":"markdown","metadata":{"id":"BWj2vz2NoXIQ"},"source":["$$f(X, \\theta) = \\sigma_0(\\sum\\theta_{w_1}\\sigma_{1}(\\sum\\theta_{w_2}\\sigma_{2}(\\ldots) + \\theta_{b_2}) + \\theta_{b_1})$$"]},{"cell_type":"markdown","metadata":{"id":"MZMYdwInoXIQ"},"source":["$$\\pi(z) = \\theta_{w_0}z + \\theta_{b_0} $$"]},{"cell_type":"markdown","metadata":{"id":"Ype8uqh0oXIR"},"source":["$$ \\mathcal{L} = \\sum(y - \\pi(z))^2$$"]},{"cell_type":"markdown","metadata":{"id":"ss8LI8yooXIR"},"source":["<center><img width=1200px src=\"https://wilocw.github.io/files/resource/global/nn_example.png\"></img></center>"]},{"cell_type":"code","metadata":{"id":"9Z51P2EqoXIR"},"source":["# again, sklearn is not the ideal library but it serves a purpose here\n","from sklearn import neural_network as nn\n","\n","def fit_nn():\n","    ''' fit a multilayer perceptrons (neural networks) of different sizes and plot '''\n","    \n","    # this is utility function that will create the neural network for a given layer architecture\n","    mlpr = lambda layers: nn.MLPRegressor(hidden_layer_sizes=layers, activation='logistic', solver='lbfgs')\n","\n","    # create plot\n","    _, axs = plt.subplots(1,3,figsize=(16, 4), sharey=True, sharex=True)\n","\n","    # different layer architectures (see sketches above !)\n","    layers = [(1,), (5,), (100,100)]\n","    # for each one, create and fit \n","    for i,layer in enumerate(layers):\n","        # create neural network\n","        mlpr_ = mlpr(layer)\n","        # train the nn\n","        mlpr_.fit(X, y)\n","        # plot the training data\n","        axs[i].plot(X, y, 'o')\n","        # plot the predicted function from the trained model\n","        axs[i].plot(nX, mlpr_.predict(nX[:,None]), label='num. params ' + str(np.sum([len(coef) for coef in mlpr_.coefs_])))\n","        # misc. details\n","        axs[i].legend(loc='upper left')\n","        axs[i].set_title('hidden layer size: ' + str(layer))\n","        axs[i].set_xlabel('X')\n","    axs[0].set_ylabel('y')\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JwE6Y41joXIR"},"source":["%matplotlib inline\n","fit_nn()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uLKAt2EmoXIS"},"source":["parameters $\\theta$ are neural network weights and biases\n","\n","hyperparameters $\\alpha$ is number and dimensionality of layers"]},{"cell_type":"markdown","metadata":{"id":"_-SxXft6oXIS"},"source":["### considerations for regression\n","\n","what are the properties of your data ? do you have lots of it ? does it have a lot of features ? do you have high dimensional output ?\n","\n","do you know anything about the trends already ? is it (mostly) linear ? is it smooth ? \n","\n","do you care about uncertainty [n.b. you should !] ?\n","\n","do easy models work fine ?"]},{"cell_type":"markdown","metadata":{"id":"wCnjqF2soXIS"},"source":["### what about classification ?"]},{"cell_type":"markdown","metadata":{"id":"i3ZDuQEToXIS"},"source":["it's just regression !"]},{"cell_type":"markdown","metadata":{"id":"sm8C4wEfoXIT"},"source":["most regression methods (sometimes _implicitly_) assume Gaussian noise in the error"]},{"cell_type":"markdown","metadata":{"id":"MCO-y_2yoXIT"},"source":["it's easier that way"]},{"cell_type":"markdown","metadata":{"id":"jIu3n9k_oXIT"},"source":["classification is just an example where the observation model $\\pi$ is not Gaussian"]},{"cell_type":"markdown","metadata":{"id":"3Ez84THpoXIT"},"source":["examples include logistic regression; categorical likelihoods (for Bayesian models) and neural networks with a softmax layer"]},{"cell_type":"code","metadata":{"id":"SIGjZAjToXIT"},"source":["# this is an example of classification\n","## we assume a latent function (z = f(X)) is a GP with a categorical likelihood (y = pi(z))\n","\n","global plot_probs # this is a bit of trickery to save classifer state outside the demo function\n","\n","def plot_classifier():\n","    ''' this will train a GP classifier and plot the decision boundaries '''\n","    \n","    # reinitialise the data \n","    data = pd.read_csv('https://wilocw.github.io/files/resource/global/2019.csv')\n","    data.rename(lambda s: s.replace(' ','_'), axis='columns', inplace=True)\n","\n","    # standardise the data (this is equiv to do it manually it's just easier this way)\n","    X = StandardScaler().fit_transform(data.loc[:,features].values)\n","    \n","    # we're going to classify the 2 principle components from earlier in the talk \n","    z = PCA(2).fit_transform(X)\n","\n","    # convert text labels into integers (1 to 6)\n","    y = np.array([{'Africa':1, 'Asia':2, 'Europe':3, 'North America':4,'Oceania':5,'South America':6}[cont] for cont in data.loc[:,'Continent']])\n","\n","    # plotting limits\n","    lims = ((-4, 5), (-3, 4))\n","\n","    # classifier model (Matern 5/2 covariance)\n","    gpc = gp.GaussianProcessClassifier(kernel=Matern(nu=5/2),n_restarts_optimizer=10).fit(z,y)\n","\n","    # create a mesh to plot 2-D classification surface on\n","    xg,yg = np.meshgrid(np.linspace(*lims[0],50),np.linspace(*lims[1],50))\n","    nX = np.vstack((xg.ravel(), yg.ravel())).T\n","\n","    # class labels for nX\n","    C = gpc.predict(nX)\n","\n","    # create a plot\n","    fig,axs = plt.subplots(1,2,figsize=(16,5), sharex=True, sharey=False)\n","    # plot the training data on the left hand plot\n","    for i in range(1,7):\n","        z_ = z[np.where(y==i),:][0,...]\n","        axs[0].plot(z_[:,0],z_[:,1],'o',label=['Africa','Asia','Europe','North America','Oceania','South America'][i-1])\n","\n","    # plot the classifier C\n","    cax = axs[1].contourf(xg, yg, C.reshape(50,50), levels=np.array([1,2,3,4,5,6,7])-0.5, colors=plt.rcParams['axes.prop_cycle'].by_key()['color'][0:6])\n","    \n","    # misc. details\n","    cbar = fig.colorbar(cax, ticks=[1,2,3,4,5,6])\n","    cbar.ax.set_yticklabels(['Africa','Asia','Europe','North America','Oceania','South America'])\n","    cbar.ax.invert_yaxis()\n","    plt.tight_layout()\n","    \n","    for ax in axs:\n","        ax.set_ylabel('pc2')\n","        ax.set_xlabel('pc1')\n","        ax.set_xlim(lims[0]), ax.set_ylim(lims[1])\n","        ax.set_aspect('equal')\n","        \n","    def _plot_probs(nX, gpc):\n","        ''' this function plots the probability surface of each class '''\n","        # predict the probability of each class for nX\n","        C = gpc.predict_proba(nX)\n","        # create a plot\n","        fig, axs = plt.subplots(2,3, figsize=(16, 11), sharex=True, sharey=True)\n","        # standardise colourmap\n","        clims = np.min(C.ravel()), np.max(C.ravel())\n","        # for each class\n","        for i in range(6):\n","            # i^th axis\n","            ax = axs[i//3, i%3]\n","            \n","            # plot the training points for that class\n","            ax.plot(z[np.where(y == i+1),0],z[np.where(y==i+1),1],'wo',label='y')\n","            \n","            # plot the probability surface for the current class (in log space)\n","            cax = ax.pcolor(xg,yg,C[:,i].reshape(50,50), norm=LogNorm(vmin=clims[0], vmax=clims[1]))\n","            \n","            # misc. details\n","            ax.set_xlim(lims[0])\n","            ax.set_title('p(' + ['Africa','Asia','Europe','North America','Oceania','South America'][i] + ')')\n","            if i%3 == 0:\n","                ax.set_ylabel('pc2')\n","            if i//3 == 1:\n","                ax.set_xlabel('pc1')\n","            ax.set_aspect('equal')\n","        plt.tight_layout()\n","        # colour bar\n","        cbar = fig.colorbar(cax, ax=axs.ravel().tolist(), shrink=0.5)\n","    \n","    ## this is a bit of trickery to save classifer state outside the demo function\n","    global plot_probs\n","    plot_probs = lambda: _plot_probs(nX, gpc) # plot_probs is a function that will evaluate this when called\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s590wDRroXIU"},"source":["%matplotlib inline\n","plot_classifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Org9mMPIoXIU"},"source":["%matplotlib inline\n","plot_probs()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9DeDW4X5oXIU"},"source":["## unsupervised learning"]},{"cell_type":"markdown","metadata":{"id":"bGc-PkJ_oXIV"},"source":["### unsupervised learning\n","\n","we have $y$ but don't know $X$\n","\n","we have $X$ but don't know $y$"]},{"cell_type":"markdown","metadata":{"id":"peNl7M1VoXIV"},"source":["already covered PCA (latent variable model): find $z, f$ to represent $X$"]},{"cell_type":"markdown","metadata":{"id":"mNfVrVm1oXIV"},"source":["### clustering\n","\n","unsupervised classification"]},{"cell_type":"markdown","metadata":{"id":"VYPYKEQZoXIV"},"source":["$$ z = f(X, \\theta)$$\n","$$ y = \\pi(z) $$"]},{"cell_type":"markdown","metadata":{"id":"C_dlN7ShoXIV"},"source":["how to define $\\mathcal{L}$ with no examples of $y$"]},{"cell_type":"markdown","metadata":{"id":"i5oVTxUYoXIW"},"source":["### k-means\n","simplest clustering method: sort into fixed number of classes, $k$, based on optimal distances from centre of clusters"]},{"cell_type":"code","metadata":{"id":"idv_A5AnoXIW"},"source":["from sklearn.cluster import KMeans\n","\n","def cluster_kmeans():\n","    ''' this example will cluster the principle components for the happiness data examples and compare continents to classes'''\n","    # reinitialise the data \n","    data = pd.read_csv('https://wilocw.github.io/files/resource/global/2019.csv')\n","    data.rename(lambda s: s.replace(' ','_'), axis='columns', inplace=True)\n","\n","    # standardise the data (this is equiv to do it manually it's just easier this way)\n","    X = StandardScaler().fit_transform(data.loc[:,features].values)\n","    \n","    # we're going to cluster the 2 principle components from earlier in the talk \n","    z = PCA(2).fit_transform(X)\n","\n","    # labels\n","    labels = ['Africa','Asia','Europe','North America','Oceania','South America']\n","    # convert label data into integers (1:6)\n","    y = np.array([{label:i+1 for i,label in enumerate(labels)}[cont] for cont in data.loc[:,'Continent']])\n","    # plotting limits\n","    lims = ((-4, 5), (-3, 4))\n","    # create a mesh to plot 2-D classification surface on\n","    xg,yg = np.meshgrid(np.linspace(*lims[0],50),np.linspace(*lims[1],50))\n","    nX = np.vstack((xg.ravel(), yg.ravel())).T\n","\n","    # create a plot\n","    fig, axs = plt.subplots(2,3, figsize=(16, 11), sharex=False, sharey=False)\n","    \n","   \n","    \n","    # kmeans with different number of clusters k\n","    n_clusters = [2, 4, 6]\n","    for i, n in enumerate(n_clusters):\n","        # cluster the data\n","        kmeans = KMeans(n_clusters=n).fit(z)\n","        # cluster plotting mesh\n","        Z = kmeans.predict(nX)\n","        # plot clustered mesh\n","        cax = axs[0,i].contourf(xg, yg, Z.reshape(50,50), levels=np.arange(7)-0.5)\n","        # plot training data (no labels)\n","        axs[0,i].plot(z[:,0],z[:,1],'o')\n","        \n","        # misc. details\n","        axs[0,i].set_xlim(lims[0])\n","        axs[0,i].set_ylim(lims[1])\n","        axs[0,i].set_xlabel('pc1')\n","        axs[0,i].set_aspect('equal')\n","\n","        # cluster training data\n","        zc = kmeans.predict(z)\n","        width=0.7/6\n","        for j in range(6):\n","            # for each class, calculate the proportion of points in each cluster \n","            counts = [np.mean(zc[np.where(y == (j+1))] == k) for k in range(n)]\n","            # then plot it in a bar chat\n","            axs[1,i].bar(np.arange(n) - (6-j-1)*width, counts, width, label=labels[j])\n","            \n","        # misc. details\n","        axs[1,i].legend()\n","        axs[1,i].set_xticks([k - 0.35 for k in range(n)])\n","        axs[1,i].set_xticklabels([str(k+1) for k in range(n)])\n","        axs[1,i].set_xlabel('cluster')\n","    plt.tight_layout()\n","\n","    axs[0,0].set_ylabel('pc2')\n","    axs[1,0].set_ylabel('proportion')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3EVnMrvAoXIW"},"source":["%matplotlib inline\n","cluster_kmeans()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3H3fLTdoXIW"},"source":["### Gaussian mixture models\n","\n","clusters now correspond to independent Gaussian distributions each indicating probability for a given $X$"]},{"cell_type":"code","metadata":{"id":"HiFgZE4BoXIW"},"source":["from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n","from matplotlib.patches import Ellipse\n","\n","def get_ellipse(m, C, col='auto'):\n","    ''' some utility code to draw an ellipse based on 95% CI of Gaussian '''\n","    v,w = np.linalg.eigh(C)\n","    v = 1.96 * np.sqrt(2.) * np.sqrt(v)\n","    u = w[0] / np.linalg.norm(w[0])\n","    angle = 180. * np.arctan2(u[1],u[0]) / np.pi\n","    if col is 'auto':\n","        ell = Ellipse(m, v[0], v[1], 180. + angle)\n","    else:\n","        ell = Ellipse(m, v[0], v[1], 180. + angle, color=col)\n","            \n","    return ell\n","\n","\n","def cluster_gmm():\n","    ''' this example applies clustering using Gaussian mixture models '''\n","    # reinitialise the data \n","    data = pd.read_csv('https://wilocw.github.io/files/resource/global/2019.csv')\n","    data.rename(lambda s: s.replace(' ','_'), axis='columns', inplace=True)\n","\n","    # standardise the data (this is equiv to do it manually it's just easier this way)\n","    X = StandardScaler().fit_transform(data.loc[:,features].values)\n","    \n","    # we're going to cluster the 2 principle components from earlier in the talk \n","    z = PCA(2).fit_transform(X)\n","\n","    # labels\n","    labels = ['Africa','Asia','Europe','North America','Oceania','South America']\n","    # convert label data into integers (1:6)\n","    y = np.array([{label:i+1 for i,label in enumerate(labels)}[cont] for cont in data.loc[:,'Continent']])\n","    # plotting limits\n","    lims = ((-4, 5), (-3, 4))\n","\n","    # create a plot\n","    fig, axs = plt.subplots(1,3, figsize=(16, 5), sharex=False, sharey=False)\n","    \n","    # kmeans with different number of gaussians in mixture\n","    n_clusters = [2, 4, 6]\n","    for i, n in enumerate(n_clusters):\n","        # get colour order (for plotting)\n","        cols = plt.rcParams['axes.prop_cycle'].by_key()['color'][0:n]\n","        \n","        # fit GMM\n","        gmm = GaussianMixture(n_components=n).fit(z)\n","        # get means and covariances of each gaussian and hard classify z\n","        ms, Cs, Z = gmm.means_, gmm.covariances_, gmm.predict(z)\n","        \n","        # plot each gaussian in model\n","        for k in range(n):\n","            # create ellipse representing 95% CI of current Gaussian\n","            ell = get_ellipse(ms[k], Cs[k], col=cols[k])\n","            ell.set_clip_box(axs[i].bbox)\n","            ell.set_alpha(0.5)\n","            axs[i].add_artist(ell)\n","            # plot the points hard classified to the current component\n","            axs[i].plot(z[Z==k,0],z[Z==k,1],'o', c=cols[k])\n","        # misc. details\n","        axs[i].set_xlim(lims[0])\n","        axs[i].set_ylim(lims[1])\n","        axs[i].set_xlabel('pc1')\n","        axs[i].set_aspect('equal')\n","\n","    plt.tight_layout()\n","    axs[0].set_ylabel('pc2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oU3el_cKoXIX"},"source":["%matplotlib inline\n","cluster_gmm()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5hIDvE6ZoXIX"},"source":["## key points"]},{"cell_type":"markdown","metadata":{"id":"pSK4UpXtoXIX"},"source":["### key points\n","\n","- most machine learning is regression"]},{"cell_type":"markdown","metadata":{"id":"fktnc5E3oXIX"},"source":["- regression is just statistics"]},{"cell_type":"markdown","metadata":{"id":"r0J3YtBJoXIY"},"source":["- machine learning is not _just_ statistics (it's optimisation)"]},{"cell_type":"markdown","metadata":{"id":"wk6Jrju-oXIY"},"source":["- no free lunch"]},{"cell_type":"markdown","metadata":{"id":"pGhYaAk-oXIY"},"source":["- learn basic maths prerequisites"]},{"cell_type":"markdown","metadata":{"id":"OhhOH00MoXIY"},"source":["- familiarise yourself with basic methods"]},{"cell_type":"markdown","metadata":{"id":"1NE4IS2IoXIY"},"source":["- rule out the easy things first"]},{"cell_type":"markdown","metadata":{"id":"k4-xrnQtoXIY"},"source":["### some resources\n"]},{"cell_type":"markdown","metadata":{"id":"GQB5znU_oXIZ"},"source":["### mathematics for machine learning (book)\n","\n","\n","<center><img src=\"https://wilocw.github.io/files/resource/global/mml-book-cover.jpg\"></img>\n","\n","### [mml-book.github.io](https://mml-book.github.io)\n","\n","</center>\n"]},{"cell_type":"markdown","metadata":{"id":"yx2OmM-qoXIZ"},"source":["### machine learning (a probabilistic perspective)\n","\n","<center><img width=600px src=\"https://wilocw.github.io/files/resource/global/ml_bookjpg.jpg\"></img>\n","\n","### [probml.github.io/pml-book](https://probml.github.io/pml-book)</center>"]},{"cell_type":"markdown","metadata":{"id":"Oo0DQmweoXIZ"},"source":["### deep learning\n","\n","<center><img src=\"https://wilocw.github.io/files/resource/global/deepbook.jpg\"></img>\n","\n","### [deeplearningbook.org](http://deeplearningbook.org)\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"tiBFoB28oXIZ"},"source":["### summer schools\n","\n","<div class='container'>\n","    \n","<div class='col' style='text-align: center'>\n","<img src=\"https://wilocw.github.io/files/resource/global/gpss.png\"></img>\n","\n","#### [gpss.cc](http://gpss.cc)\n","    \n","</div>\n","<div class='col' style='text-align: center'>\n","<img src=\"https://wilocw.github.io/files/resource/global/mlss.png\"></img>\n","\n","</div>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"J_QaMxcSoXIZ"},"source":["### [github.com](https://github.com)\n","\n","<center><img src=\"https://wilocw.github.io/files/resource/global/github.png\"></img></center>"]},{"cell_type":"markdown","metadata":{"id":"EFuVgGY4oXIa"},"source":["### [paperswithcode.com](https://paperswithcode.com)\n","\n","<center><img src=\"https://wilocw.github.io/files/resource/global/pwc.png\"></img></center>"]},{"cell_type":"markdown","metadata":{"id":"WOk7IcC0oXIa"},"source":["### [arxiv.org](arxiv.org)\n","\n","<div class='container'>\n","    \n","<div class='col' style='text-align: center'>\n","<img src=\"https://wilocw.github.io/files/resource/global/arxivml.png\"></img>\n","</div>\n","<div class='col' style='text-align: center'>\n","<img src=\"https://wilocw.github.io/files/resource/global/arxivlg.png\"></img>\n","</div>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"EI9GlbAJoXIa"},"source":["## \"hang on, i asked about ...\""]},{"cell_type":"markdown","metadata":{"id":"POMlrDMvoXIa"},"source":["### graphs\n","\n","graphs are non-trivial structures; inference has same basic structure\n","\n","$$ G(X; W) = (V, E) $$\n","$$ z = f(G(X), W)$$\n","$$ y = \\pi(z)$$"]},{"cell_type":"markdown","metadata":{"id":"FdtRHD7ioXIa"},"source":["general ideas and problem categories are the same; methods need to be adapted to network, e.g. on embeddings, adjacency matrices"]},{"cell_type":"markdown","metadata":{"id":"pOykpaXqoXIa"},"source":["<div class='container'>\n","    \n","<div class='col' style='text-align: center'>\n","<img src=\"https://wilocw.github.io/files/resource/global/graph_taxonomy.PNG\"></img>\n","</div>\n","<div class='col' style='text-align: center'>\n","<img src=\"https://wilocw.github.io/files/resource/global/graph_review.PNG\"></img>\n","</div>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"cShnMZBXoXIb"},"source":["<center><img src=\"https://wilocw.github.io/files/resource/global/pwc_graphs.png\"></img>\n","\n","https://paperswithcode.com/area/graphs\n","</center>"]},{"cell_type":"markdown","metadata":{"id":"xiIh0A-KoXIb"},"source":["### 3-D data\n","\n","what do you mean ?\n","\n","3-D spatial data ? 3-D volumes, e.g. tomography ?\n","\n","does a regular method work ? GLMs ? GPs ?"]},{"cell_type":"markdown","metadata":{"id":"L2cmwTqaoXIb"},"source":["### spatial data\n","\n","spatial data is multi-format\n","\n","is it sparsely distributed over a grid ?\n","\n","can it be modelled as a network ?\n","\n","spatial statistics can be hard\n","spatio-temporal statistics is harder"]},{"cell_type":"markdown","metadata":{"id":"9PnNrMj9oXIb"},"source":["<div class='container'>\n","    \n","<div class='col' style='text-align: center'>\n","<img src=\"https://wilocw.github.io/files/resource/global/cressie.jpg\"></img>\n","</div>\n","<div class='col' style='text-align: center'>\n","<img src=\"https://wilocw.github.io/files/resource/global/cressiewikle.jpg\"></img>\n","</div>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"8Dz3rGumoXIb"},"source":["### Bayesian model selection\n","\n","this is about automatically searching the trade-offs discussed\n","\n","read up on information theory !\n","\n","information criteria (BIC) used to choose between models\n","\n","automatic statistician"]}]}